---
title: "HLS: Your new favorite remote sensing dataset"
date: "2/24/2023"
date-modified: "3/20/2023"
image: "hls.png"
fig-cap-location: top
---

::: {.column-page layout="[[1, 1], [-0.25, 0.5, -0.25]]"}
![Sentinel 2 L2A](index_files/figure-html/fig-mosaic-comparison-output-1.png)

![Landsat Collection 2 Level-2](index_files/figure-html/fig-mosaic-comparison-output-2.png)

![Harmonized Landsat Sentinel 2](index_files/figure-html/fig-mosaic-comparison-output-3.png)
:::


The [Harmonized Landsat Sentinel 2 (HLS) dataset](https://lpdaac.usgs.gov/data/get-started-data/collection-overview/missions/harmonized-landsat-sentinel-2-hls-overview/)
is an awesome public source for 30 meter spectral satellite imagery.

Produced by NASA, it combines the independently powerful Landsat 8/9
and Sentinel 2 satellite observation systems into one analysis-ready dataset.
Some of the best features include:

* More rich time series than either Sentinel 2 or Landsat individually

  * one observation every two to three days from HLS compared to one every five
days for Sentinel 2 and one every eight days for Landsat 8/9

* It is
[BRDF corrected](https://en.wikipedia.org/wiki/Bidirectional_reflectance_distribution_function),
which makes mosaics generated across orbits from different days nearly seamless

    * neither of the original data sources for Landsat or Sentinel data have
this feature

* The data are fairly easily accessible via NASA's
[LPCLOUD STAC](https://cmr.earthdata.nasa.gov/stac/LPCLOUD)!

* Improved cloud masking for Sentinel 2!
    
    * The original Sentinel 2 cloud mask leaves something to be desired, but
in the HLS collection you get a more robust mask that is consistent with the
Landsat products

In this post, I am going to demonstrate some of the awesome capabilities of the
HLS data and show why you should consider using it in place of either Landsat
or Sentinel 2!
Check out the
[HLS 2.0 User Guide](https://lpdaac.usgs.gov/documents/1326/HLS_User_Guide_V2.pdf)
for more details on the process for generating the HLS data.

## Get organized

### authentication with Earthdata and the .netrc file
To access the HLS data from NASA, you will need to have a (free) Earthdata Login
profile.
You can create an account [here](https://urs.earthdata.nasa.gov/).
Once your account is active, you will need to add your login credentials to a
`.netrc` file in your home directory.
You can do this with a single shell command in a Unix system:
```sh
echo "machine urs.earthdata.nasa.gov login <USERNAME> password <PASSWORD>" > ~/.netrc
```
When GDAL reads the raster data using `curl` it will find your credentials in
this file.

### environment variables
There are a handful of environment variables that must be set in order for GDAL
to locate and read the raster files correctly.
You can choose to include these in a startup script (e.g. `~/.bashrc`) or set
them within your Python script:

```sh
CPL_VSIL_CURL_USE_HEAD=FALSE
GDAL_DISABLE_READDIR_ON_OPEN=YES
GDAL_HTTP_COOKIEJAR=/tmp/cookies.txt
GDAL_HTTP_COOKIEFILE=/tmp/cookies.txt
```

### imports
Here are the python packages that we need:
```{python}
import datetime
import itertools
from collections import defaultdict

import pandas as pd
import pyproj
import pystac
import pystac_client
import stackstac
import xarray as xr
from rasterio.enums import Resampling
from shapely.geometry import box
from shapely.ops import transform

```

### connect to the NASA STAC
The HLS data queried from the LPCLOUD STAC.
We can use `pystac_client` to open a connection to the STAC:
```{python}
CMR_STAC_URL = "https://cmr.earthdata.nasa.gov/stac/LPCLOUD"

catalog = pystac_client.Client.open(CMR_STAC_URL)
```

The HLS data are stored in two separate collections: 

* Landsat: `HLSL30.v2.0`

* Sentinel: `HLSS30.v2.0`

```{python}
HLS_COLLECTION_IDS = ["HLSL30.v2.0", "HLSS30.v2.0"]
```
There may be good reasons to keep them separate but this difference and one
other design choice that we will discuss later make it more tedious to get up
and running with the HLS data than it should!

### area of interest
For this analysis we are going to focus on a section of the Kawishiwi River
system on the edge of the Boundary Waters Canoe Area Wilderness in Northern
Minnesota that I know as "the Little Triangle".
 
```{python}
# my CRS of choice for CONUS these days
CRS_STRING = "epsg:5070"
EPSG = pyproj.CRS.from_string(CRS_STRING).to_epsg()

# bounding box that surrounds the Little Triangle
AOI = box(326000, 2771000, 337000, 2778000)

# STAC items store bounding box info in epsg:4326
transformer_4326 = pyproj.Transformer.from_crs(
    crs_from=CRS_STRING,
    crs_to="epsg:4326",
    always_xy=True,
)

bbox_4326 = transform(transformer_4326.transform, AOI).bounds
```

## Explore the data

### check out this time series!
We can query the STAC catalog for the entire time series by searching the
catalog without a `datetime` specification.
I am looping through the `pages()` because the NASA STAC connection can be flaky
sometimes and will give you 502 errors when you loop through a long list of
`items()`.
I don't know why the `pages()` method is more stable, but it seems to work
better for large queries like this.
```{python}
hls_history_search = catalog.search(
    collections=HLS_COLLECTION_IDS,
    bbox=bbox_4326,
)

all_items = []
for page in hls_history_search.pages():
    all_items.extend(page.items)

collection_history = {
    collection: defaultdict(list) for collection in HLS_COLLECTION_IDS
}

for item in all_items:
    year_month = pd.Timestamp(item.datetime.date()) + pd.offsets.MonthEnd()
    entry = collection_history[item.collection_id][year_month]
    if (date := item.datetime.date()) not in entry:
        entry.append(date)
```

```{python}
# | code-fold: true
# get count of images by year/month/sensor
collection_counts = pd.DataFrame(
    {
        collection: {
            year_month: len(dates) for year_month, dates in year_months.items()
        }
        for collection, year_months in collection_history.items()
    }
).fillna(0)

_ = collection_counts.plot.line(
    title="monthly count of days with observations by sensor"
)
```
You can see that the Landsat time series stretches back to Landsat 8's launch in
2013 and that the Sentinel 2 observations kick into gear in late 2020.
I think the plan is to add the full Sentinel 2 archive to the HLS dataset but
that is a work in progress.
Things really got kicked up a notch in 2022 when Landsat 9 came online!

### band labels
One thing that makes it harder to get up and running with the HLS data is that
the Sentinel and Landsat STAC items have different band IDs for the many of the
spectral bands!
This matters to us here because `stackstac` uses the STAC items' asset labels
to name the `band` dimension.
Without any modifications, some of the bands would get scrambled when combining
data from the two collections.

```{python}
# | code-fold: true
import camelot

tables = camelot.read_pdf(
    "https://lpdaac.usgs.gov/documents/1326/HLS_User_Guide_V2.pdf", pages="6"
)
band_df = tables[0].df
band_df.columns = band_df.iloc[0].str.replace("\n", "")
band_df = band_df.drop(band_df.index[0])
band_df
```

To avoid this headache, we can make define a crosswalk dictionary that will make
it possible to modify the STAC item metadata so that we can safely load all of
the data without scrambling the bands.

```{python}
BAND_CROSSWALK = {
    "HLSL30.v2.0": {
        "B01": "coastal aerosol",
        "B02": "blue",
        "B03": "green",
        "B04": "red",
        "B05": "nir narrow",
        "B06": "swir1",
        "B07": "swir2",
        "B09": "cirrus",
        "B10": "thermal infrared 1",
        "B11": "thermal",
    },
    "HLSS30.v2.0": {
        "B01": "coastal aerosol",
        "B02": "blue",
        "B03": "green",
        "B04": "red",
        "B05": "red-edge 1",
        "B06": "red-edge 2",
        "B07": "red-edge 3",
        "B08": "nir broad",
        "B8A": "nir narrow",
        "B09": "water vapor",
        "B10": "cirrus",
        "B11": "swir 1",
        "B12": "swir 2",
    },
}

# these are the ones that we are going to use
BANDS = ["red", "green", "blue", "Fmask"]
```

## Make a cloud-free mosaic

### search the STAC
Now we are going to search the STAC again but this time with a `datetime`
specification to limit our search to July 2022.
```{python}
START_DATE = "2022-07-01"
END_DATE = "2022-07-31"

stac_items = catalog.search(
    collections=HLS_COLLECTION_IDS,
    bbox=bbox_4326,
    datetime=[START_DATE, END_DATE],
).get_all_items()
```

### modify the asset labels
Now we use the crosswalk to rename the STAC item asset labels to pave the way
for `stackstac` to load the data in the best way.
There are ways to do this after you load the data using
`xarray.DataArray.assign_coords`, but I find this approach more intuitive.
```{python}
for item in stac_items:
    for original_band, new_band in BAND_CROSSWALK.get(item.collection_id).items():
        item.assets[new_band] = item.assets.pop(original_band)


```

### cloud/shadow masking
The `Fmask` band contains all of the pixel quality information that we need to
mask out invalid pixels from the images, but it takes a little bit of work.
The quality information is stored in integer form that needs to get unpacked
into bit-wise values to get binary values for seven attributes
See Appendix A of the
[HLS User Guide](https://lpdaac.usgs.gov/documents/1326/HLS_User_Guide_V2.pdf)
for more details.

Here is a function that we can use to get all of the integer values that meet
several quality criteria:
```{python}
def get_valid_fmask_ints(valid_conditions: dict):
    # enumerate all possible eight digit binary values
    all_binary_values = list(itertools.product([0, 1], repeat=8))

    mapping = {
        "aerosol": slice(0, 2),
        "water": 2,
        "snow_ice": 3,
        "cloud_shadow": 4,
        "adjacent_to_cloud_shadow": 5,
        "cloud": 6,
        "cirrus": 7,
    }

    valid_ints = []
    for binary_value in all_binary_values:
        n_acceptable = 0
        for key, acceptable_value in valid_conditions.items():
            if binary_value[mapping[key]] == acceptable_value:
                n_acceptable += 1
        # if all of the valid constraints are met, add this one to the list!
        if n_acceptable == len(valid_conditions):
            binary_str = "".join([str(val) for val in binary_value])
            # use the int function with base=2 to convert the binary string
            # to an integer
            valid_ints.append(int(binary_str, base=2))

    return valid_ints


```

Get the integer values that represent pixels that are not clouds,
cloud shadows, or adjacent to clouds or shadows:
```{python}
valid_fmask_ints = get_valid_fmask_ints(
    valid_conditions={"cloud_shadow": 0, "adjacent_to_cloud_shadow": 0, "cloud": 0}
)
```

### get the cloud-free mosaic
It is very easy to generate a cloud-free mosaic for a specific time range
(e.g. monthly).
For example, we can take the median value from all valid pixels within a given
month with just a few lines of code.
We are only looking at a single month here, but you could pass many months worth
of STAC items to `stackstac.stack` and get a rich time series of cloud-free
imagery using the exact same code.
```{python}
hls_stack_raw = stackstac.stack(
    stac_items,
    assets=BANDS,
    bounds=AOI.bounds,
    epsg=EPSG,
    resolution=30,
    xy_coords="center",
)

hls_cloud_free = (
    hls_stack_raw.where(hls_stack_raw.sel(band="Fmask").isin(valid_fmask_ints))
    .resample(time="1M")
    .median(skipna=True)
    .compute()
)
```

Plot the image and bask in the cloud-free beauty:
```{python}
# | warning: false
# | message: false
# | code-fold: true
# | column: page
# | fig-align: center
# | layout: "[-0.1, 0.8, -0.1]"
_ = (
    hls_cloud_free.squeeze(dim="time")
    .sel(band=["red", "green", "blue"])
    .plot.imshow(
        rgb="band",
        robust=True,
        size=6,
        add_labels=False,
        xticks=[],
        yticks=[],
    )
)

```

## Compare to Sentinel 2 L2A and Landsat Collection 2 Level-2

```{python original_sentinel}
# | code-fold: true
# | warning: false
# | message: false
import planetary_computer

element84_catalog = pystac_client.Client.open(
    "https://earth-search.aws.element84.com/v0"
)
sentinel_stac_items = element84_catalog.search(
    collections=["sentinel-s2-l2a-cogs"],
    bbox=bbox_4326,
    datetime=[START_DATE, END_DATE],
).get_all_items()

sentinel_band_crosswalk = {
    "B04": "red",
    "B03": "green",
    "B02": "blue",
}

for item in sentinel_stac_items:
    for original_band, new_band in sentinel_band_crosswalk.items():
        item.assets[new_band] = item.assets.pop(original_band)


sentinel_stack = stackstac.stack(
    items=sentinel_stac_items,
    assets=["red", "green", "blue", "SCL"],
    epsg=EPSG,
    resolution=30,
    bounds=AOI.bounds,
    xy_coords="center",
    resampling=Resampling.bilinear,
)

# valid SCL values: 4: vegetation, 5: bare soils, 6: water, 11: snow or ice
# https://docs.digitalearthafrica.org/en/latest/data_specs/Sentinel-2_Level-2A_specs.html
valid_scl = [4, 5, 6, 11]

sentinel_cloud_free = (
    sentinel_stack.where(sentinel_stack.sel(band="SCL").isin(valid_scl))
    .sel(band=["red", "green", "blue"])
    .resample(time="1M")
    .median(skipna=True)
    .compute()
)

planetary_computer_catalog = pystac_client.Client.open(
    "https://planetarycomputer.microsoft.com/api/stac/v1",
    modifier=planetary_computer.sign_inplace,
)
landsat_stac_items = planetary_computer_catalog.search(
    collections=["landsat-c2-l2"],
    bbox=bbox_4326,
    datetime=[START_DATE, END_DATE],
).get_all_items()

landsat_stack = stackstac.stack(
    items=landsat_stac_items,
    assets=["red", "green", "blue", "qa_pixel"],
    epsg=EPSG,
    resolution=30,
    bounds=AOI.bounds,
    xy_coords="center",
)

# following https://stackstac.readthedocs.io/en/latest/examples/gif.html#Mask-cloudy-pixels-using-the-QA-band:
# Make a bitmask---when we bitwise-and it with the data, it leaves just the 4 bits we care about
mask_bitfields = [1, 2, 3, 4]  # dilated cloud, cirrus, cloud, cloud shadow
bitmask = 0
for field in mask_bitfields:
    bitmask |= 1 << field

landsat_qa = landsat_stack.sel(band="qa_pixel").astype("uint16")
landsat_bad = landsat_qa & bitmask  # just look at those 4 bits

landsat_good = landsat_stack.where(
    landsat_bad == 0
)  # mask pixels where any one of those bits are set

landsat_cloud_free = (
    landsat_good.sel(band=["red", "green", "blue"])
    .resample(time="1M")
    .median(skipna=True)
    .compute()
)

```

Both of the original datasets are atmospherially corrected and represent surface
surface reflectance, but the processed images are definitely not free of cloud
artifacts!
One of the greatest advantages of using the HLS dataset instead of the original
sources is improved cloud masking.
The HLS data undergo an entirely different process than the Landsat Collection
Level-2 and Sentinel 2 Level 2A datasets, see below for an outline of the
processing workflow.

![](https://www.earthdata.nasa.gov/s3fs-public/styles/medium_half_480px_/public/imported/HLS_Infographic_rev.jpg?VersionId=tUoXm2ru6N6AI4DEr0Wi7a2j5jnqs07k&itok=khHqtvQ7)

These images show 30 meter resolution, cloud-free mosaics for July 2022 from
each source:

```{python}
# | echo: false
# | message: false
# | warning: false
# | label: fig-mosaic-comparison
# | column: page
# | layout: [[1, 1], [-0.25, 0.5, -0.25]]
# | fig-cap:
# |   - "Sentinel 2"
# |   - "Landsat 8/9"
# |   - "Harmonized"

for stack in [sentinel_cloud_free, landsat_cloud_free, hls_cloud_free]:
    _ = (
        stack.squeeze(dim="time")
        .sel(band=["red", "green", "blue"])
        .plot.imshow(
            rgb="band",
            robust=True,
            size=5,
            add_labels=False,
            xticks=[],
            yticks=[],
        )
    )

```

In the Sentinel 2 image you can clearly see some cloud haze scattered around
most of the image.
The Landsat image shows some stripy artifacts that are probably attributable to
hazy clouds that were not masked from the original images.
Meanwhile, the HLS mosaic comes out crystal clear!

## Analyze the Landsat and Sentinel collections separately
To illustrate the power of the full HLS dataset and the advantage of the
high density of collections in a short time, lets load the imagery for each
sensor separately and create a cloud-free mosaic for July 2022 for each one.

### get an array for each sensor

```{python}

items_by_collection = {
    collection: pystac.ItemCollection(
        [item for item in stac_items if item.collection_id == collection]
    )
    for collection in HLS_COLLECTION_IDS
}

stacks_by_collection = {}


# a function to flatten the time dimension
def flatten(x, dim="time"):
    assert isinstance(x, xr.DataArray)
    if len(x[dim].values) > len(set(x[dim].values)):
        x = x.groupby(dim).map(stackstac.mosaic)

    return x


for collection, collection_items in items_by_collection.items():
    raw_stack = stackstac.stack(
        collection_items,
        assets=BANDS,
        bounds=AOI.bounds,
        epsg=EPSG,
        resolution=30,
        xy_coords="center",
    )

    stacks_by_collection[collection] = flatten(raw_stack, dim="time")

```

We can combine them along a new dimension called `sensor` to make it easy to
look at the images with respect to sensor and time.
```{python}
sensor_stack = xr.concat(
    list(stacks_by_collection.values()),
    dim=pd.Index(["Landsat", "Sentinel"], name="sensor"),
)
sensor_stack
```

Since we are going to make several plots using this array, load the data into
memory with a call to the `.compute()` method.
```{python}
sensor_stack = sensor_stack.compute()


```

The HLS data are organized by 100 km MGRS tiles, and there can be redundant 
coverage by separate STAC items.
We can clean this up by flattening the data out to have one observation per
sensor per day
```{python}
# a function to prepare the arrays for better plotting
def flatten_by_day(x):
    return (
        x.assign_coords(time=x.time.astype("datetime64[D]"))
        .groupby("time")
        .map(stackstac.mosaic)
    )


flattened_by_day = xr.concat(
    [
        flatten_by_day(sensor_stack.sel(sensor=sensor))
        for sensor in ["Landsat", "Sentinel"]
    ],
    dim="sensor",
)
```

Now when we plot the images we only have one observation per day per sensor.
```{python}
# | column: page
# | warning: false
# | message: false
_ = flattened_by_day.sel(band=["red", "green", "blue"]).plot.imshow(
    col="sensor",
    row="time",
    rgb="band",
    robust=True,
    size=5,
    vmin=0,
    vmax=800,
    add_labels=False,
    xticks=[],
    yticks=[],
)
```

Now let's look at the raw `Fmask` values for all of the images.
```{python}
# | column: page
# | warning: false
# | message: false
fmask = flattened_by_day.sel(band=["Fmask"]).squeeze(dim="band")
_ = fmask.sel(sensor=["Landsat", "Sentinel"]).plot.imshow(
    col="sensor",
    row="time",
    cmap="tab20",
    size=4,
    add_labels=False,
    xticks=[],
    yticks=[],
)
```

We could interpret the values individually, but it looks like the dark blue and
peach categories represent non-cloudy land and water respectively.


Check out the valid/invalid binary values for each sensor/date:
```{python}
# | warning: false
# | message: false
# | column: page
_ = fmask.isin(valid_fmask_ints).plot.imshow(
    col="sensor",
    row="time",
    size=4,
    add_labels=False,
    xticks=[],
    yticks=[],
)
```

For illustration purposes, make a "combined" sensor array to add to `sensor_stack`
```{python}
landsat = (
    sensor_stack.sel(sensor="Landsat")
    .assign_coords(sensor="combined")
    .expand_dims("sensor", 1)
)
sentinel = (
    sensor_stack.sel(sensor="Sentinel")
    .assign_coords(sensor="combined")
    .expand_dims("sensor", 1)
)
combined_stack = landsat.combine_first(sentinel)

full_stack = xr.concat([sensor_stack, combined_stack], dim="sensor")
```

Would we have at least one valid observation for every pixel?
```{python}
# | warning: false
# | message: false
# | column: page
has_valid = full_stack.sel(band="Fmask").isin(valid_fmask_ints).any(dim="time")
_ = has_valid.plot.imshow(
    col="sensor",
    size=4,
    add_labels=False,
    vmin=0,
    vmax=1,
    xticks=[],
    yticks=[],
)
```
Not if we only use Landsat images, not if we use only Sentinel images, but we
nearly have full coverage if we use the combined set!

```{python}
(has_valid < 1).sum(dim=["x", "y"])
```

By combining the sensors we reduce the number of pixels with no valid
observations from hundreds down to only three!


Get cloud-free mosaic for each sensor and the combined set side-by-side.
```{python}
# | warning: false
# | message: false
# | column: page
sensor_cloud_free = full_stack.where(
    full_stack.sel(band="Fmask").isin(valid_fmask_ints)
).median(dim="time", skipna=True)

_ = sensor_cloud_free.sel(band=["red", "green", "blue"]).plot.imshow(
    col="sensor",
    rgb="band",
    robust=True,
    size=4,
    vmin=0,
    vmax=800,
    add_labels=False,
    xticks=[],
    yticks=[],
)

```

If you look closely, you can see some cloud artifacts in each of the Landsat-
and Sentinel-only panels, but the mosaic derived from both data sources looks
totally cloud-free.
That is why you are going to love HLS.
